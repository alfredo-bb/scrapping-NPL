# -*- coding: utf-8 -*-
"""scrapping +NPL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dM24nT7mBks0JhKHJj7Z1zLX7xbax_Jr
"""

import requests as rq
from bs4 import BeautifulSoup
import pandas as pd

pag="https://revistas.usantotomas.edu.co/index.php/cife/issue/archive"

rq.get(pag)

pagina=rq.get(pag)

pagina_soup=BeautifulSoup(pagina.text,"lxml")

pagina_soup.find("h2",attrs={"class":"media-heading"}).a.get("href")

#The "a" means that we are inside the "a" container

all_pages=pagina_soup.find_all("h2",attrs={"class":"media-heading"})

links=[]

for i in all_pages:
  links.append(i.a.get("href"))

links

links[0]

magazine=rq.get(links[0])

magazine_soup=BeautifulSoup(magazine.text,"lxml")

magazine_soup.find("h3",attrs={"class":"media-heading"}).a.get("href")

#as the articles are inside an H3, we call the h3

all_articles=magazine_soup.find_all("h3",attrs={"class":"media-heading"})

articles_links=[]

for i in all_articles:
  articles_links.append(i.a.get("href"))

articles_links

article=rq.get(articles_links[1])

article_soup=BeautifulSoup(article.text,"lxml")

article_soup.find("h2",attrs={"class":"page-header"}).text

article_soup.find("div",attrs={"class":"authors"}).text

article_soup.find("div",attrs={"class":"article-abstract"}).text

data={"title":[],
      "authors":[],
      "abstract":[]}

for i in links[:2]:
  magazine=rq.get(i)
  magazine_soup=BeautifulSoup(magazine.text,"lxml")
  all_articles=magazine_soup.find_all("h3",attrs={"class":"media-heading"})
  articles_links=[i.a.get("href") for i in all_articles]
  for j in articles_links:
    article=rq.get(j) 
    article_soup=BeautifulSoup(article.text,"lxml")
    data["title"].append(article_soup.find("h2",attrs={"class":"page-header"}).text)
    data["authors"].append(article_soup.find("div",attrs={"class":"authors"}).text)
    data["abstract"].append(article_soup.find("div",attrs={"class":"article-abstract"}).text)

df=pd.DataFrame(data)

pip install contexto

import contexto

help(contexto)

from contexto.limpieza import limpieza_texto,lista_stopwords

texto=df.loc[1,"abstract"]

#1 means the row, abstract means the column

stop_words=lista_stopwords()

texto

texto_limpio=limpieza_texto(texto,n_min=3,lista_palabras=stop_words)

#we take out the words that are not relevant, for example prepositions,articles,words with less than 3 characters etc...these words are calles stop words.

#steaming: we extract the root of a word . Ex: niño/niña/niños/niñas---->niñ
#lematization: we extract the "lemas". Ex: niño/niña/niños/niñas---->niño

import stanza

stanza.download("es")

nlp=stanza.Pipeline(lang="es",processors="tokenize,lemma")

doc=nlp(texto_limpio)

texto_lemma=[]

for sent in doc.sentences:
  for word in sent.words:
    texto_lemma.append(word.lemma)

texto_lemma=" ".join(texto_lemma)

#we create a big string with all the words

texto_lemma

from contexto.exploracion import par_nubes

pip install matplotlib==3.1

par_nubes(texto_lemma)

#we transform the text into numbers

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

#CountVectorizer count how many times a word is repeated in the document
#TfidfVectorizer penalises the words that doesnt repeat too much and gives points to the word that repeat many times

lista=[texto_lemma]

vec=CountVectorizer()

vec2=vec.fit_transform(lista)

vec2.toarray()

vec.get_feature_names_out()













